# -*- coding: utf-8 -*-
"""DDOS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xXLNxxewRJGRDqvS6-Bv53IltNrownFI
"""

!pip install pyswarm

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, StackingClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from pyswarm import pso
from numpy.linalg import inv
from sklearn.base import BaseEstimator, ClassifierMixin

class ELMClassifier(BaseEstimator, ClassifierMixin):
    def __init__(self, n_hidden=100, activation='sigmoid', random_state=None, reg_lambda=1e-3):
        self.n_hidden = n_hidden
        self.activation = activation
        self.random_state = random_state
        self.reg_lambda = reg_lambda

    def _activation(self, X):
        if self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-X))
        elif self.activation == 'tanh':
            return np.tanh(X)
        elif self.activation == 'relu':
            return np.maximum(0, X)
        else:
            raise ValueError("Unsupported activation function.")

    def fit(self, X, y):
        if self.random_state is not None:
            np.random.seed(self.random_state)
        n_features = X.shape[1]
        self.input_weights_ = np.random.randn(self.n_hidden, n_features)
        self.biases_ = np.random.randn(self.n_hidden)
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        self.classes_ = np.unique(y)
        HTH = np.dot(H.T, H)
        reg = self.reg_lambda * np.eye(HTH.shape[0])
        self.output_weights_ = np.dot(inv(HTH + reg), np.dot(H.T, y))
        return self

    def predict(self, X):
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        output = np.dot(H, self.output_weights_)
        return (output >= 0.5).astype(int)

    def predict_proba(self, X):
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        output = np.dot(H, self.output_weights_)
        prob_pos = 1 / (1 + np.exp(-output))
        return np.vstack([1 - prob_pos, prob_pos]).T

# ---------------------------
# ELM with PSO Optimization (ELM+PSO)
# ---------------------------
class ELM_PSO(ELMClassifier):
    def fit(self, X, y):
        n_features = X.shape[1]
        n_params = self.n_hidden * n_features + self.n_hidden
        lb_params = [-1] * n_params
        ub_params = [1] * n_params

        def objective(p):
            w = p[:self.n_hidden * n_features].reshape(self.n_hidden, n_features)
            b = p[self.n_hidden * n_features:].reshape(self.n_hidden)
            H = self._activation(np.dot(X, w.T) + b)
            HTH = np.dot(H.T, H)
            reg = self.reg_lambda * np.eye(HTH.shape[0])
            out_w = np.dot(inv(HTH + reg), np.dot(H.T, y))
            preds = (np.dot(H, out_w) >= 0.5).astype(int)
            return np.mean(preds != y)

        opt_params, best_obj = pso(objective, lb_params, ub_params, swarmsize=100, maxiter=50)
        for _ in range(50):
            candidate = opt_params + np.random.uniform(-0.05, 0.05, size=n_params)
            candidate_obj = objective(candidate)
            if candidate_obj < best_obj:
                best_obj = candidate_obj
                opt_params = candidate.copy()
        self.input_weights_ = np.array(opt_params[:self.n_hidden * n_features]).reshape(self.n_hidden, n_features)
        self.biases_ = np.array(opt_params[self.n_hidden * n_features:])
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        HTH = np.dot(H.T, H)
        reg = self.reg_lambda * np.eye(HTH.shape[0])
        self.output_weights_ = np.dot(inv(HTH + reg), np.dot(H.T, y))
        self.classes_ = np.unique(y)
        return self

# ---------------------------
# ELM with WOA Optimization (ELM+WOA)
# ---------------------------
class ELM_WOA(ELMClassifier):
    def fit(self, X, y):
        n_features = X.shape[1]
        n_params = self.n_hidden * n_features + self.n_hidden
        best_params = np.random.uniform(-1, 1, n_params)
        best_obj = float('inf')
        iterations = 100
        for _ in range(iterations):
            candidate = best_params + np.random.uniform(-0.1, 0.1, size=n_params)
            w = candidate[:self.n_hidden * n_features].reshape(self.n_hidden, n_features)
            b = candidate[self.n_hidden * n_features:].reshape(self.n_hidden)
            H = self._activation(np.dot(X, w.T) + b)
            HTH = np.dot(H.T, H)
            reg = self.reg_lambda * np.eye(HTH.shape[0])
            out_w = np.dot(inv(HTH + reg), np.dot(H.T, y))
            preds = (np.dot(H, out_w) >= 0.5).astype(int)
            candidate_obj = np.mean(preds != y)
            if candidate_obj < best_obj:
                best_obj = candidate_obj
                best_params = candidate.copy()
        for _ in range(50):
            candidate = best_params + np.random.uniform(-0.05, 0.05, size=n_params)
            w = candidate[:self.n_hidden * n_features].reshape(self.n_hidden, n_features)
            b = candidate[self.n_hidden * n_features:].reshape(self.n_hidden)
            H = self._activation(np.dot(X, w.T) + b)
            HTH = np.dot(H.T, H)
            reg = self.reg_lambda * np.eye(HTH.shape[0])
            out_w = np.dot(inv(HTH + reg), np.dot(H.T, y))
            preds = (np.dot(H, out_w) >= 0.5).astype(int)
            candidate_obj = np.mean(preds != y)
            if candidate_obj < best_obj:
                best_obj = candidate_obj
                best_params = candidate.copy()
        self.input_weights_ = np.array(best_params[:self.n_hidden * n_features]).reshape(self.n_hidden, n_features)
        self.biases_ = np.array(best_params[self.n_hidden * n_features:])
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        HTH = np.dot(H.T, H)
        reg = self.reg_lambda * np.eye(HTH.shape[0])
        self.output_weights_ = np.dot(inv(HTH + reg), np.dot(H.T, y))
        self.classes_ = np.unique(y)
        return self

# ---------------------------
# Hybrid ELM with PSO+WOA Optimization (ELM+PSO+WOA)
# ---------------------------
class ELM_PSO_WOA(ELMClassifier):
    def fit(self, X, y):
        n_features = X.shape[1]
        n_params = self.n_hidden * n_features + self.n_hidden
        lb_params = [-1] * n_params
        ub_params = [1] * n_params

        def objective(p):
            w = p[:self.n_hidden * n_features].reshape(self.n_hidden, n_features)
            b = p[self.n_hidden * n_features:].reshape(self.n_hidden)
            H = self._activation(np.dot(X, w.T) + b)
            HTH = np.dot(H.T, H)
            reg = self.reg_lambda * np.eye(HTH.shape[0])
            out_w = np.dot(inv(HTH + reg), np.dot(H.T, y))
            preds = (np.dot(H, out_w) >= 0.5).astype(int)
            return np.mean(preds != y)

        best_params, best_obj = pso(objective, lb_params, ub_params, swarmsize=150, maxiter=70)
        for _ in range(50):
            candidate = best_params + np.random.uniform(-0.1, 0.1, size=n_params)
            candidate_obj = objective(candidate)
            if candidate_obj < best_obj:
                best_obj = candidate_obj
                best_params = candidate.copy()
        for _ in range(50):
            candidate = best_params + np.random.uniform(-0.05, 0.05, size=n_params)
            candidate_obj = objective(candidate)
            if candidate_obj < best_obj:
                best_obj = candidate_obj
                best_params = candidate.copy()
        self.input_weights_ = np.array(best_params[:self.n_hidden * n_features]).reshape(self.n_hidden, n_features)
        self.biases_ = np.array(best_params[self.n_hidden * n_features:])
        H = self._activation(np.dot(X, self.input_weights_.T) + self.biases_)
        HTH = np.dot(H.T, H)
        reg = self.reg_lambda * np.eye(HTH.shape[0])
        self.output_weights_ = np.dot(inv(HTH + reg), np.dot(H.T, y))
        self.classes_ = np.unique(y)
        return self

# ---------------------------
# Global PSO Objective Function (for Feature Selection)
# ---------------------------
def pso_objective(mask, X_data, y_data):
    mask = np.round(mask).astype(int)
    if np.sum(mask) == 0:
        return 1.0
    X_sel = X_data[:, mask == 1]
    clf = RandomForestClassifier(n_estimators=50, random_state=42)
    clf.fit(X_sel, y_data)
    y_pred = clf.predict(X_sel)
    return 1 - accuracy_score(y_data, y_pred)
# ---------------------------
# Load and Preprocess Data (unchanged)
# ---------------------------
# ---------------------------
# Load, clean, map, encode, and then sample exactly 1000 per class
# ---------------------------

# Read CSV files with error handling for malformed rows
train_df = pd.read_csv("/content/train_set.csv")
test_df  = pd.read_csv("/content/test_set.csv", on_bad_lines='skip')  # skips problematic rows

# Drop any “Unnamed” index columns and strip whitespace
train_df = train_df.loc[:, ~train_df.columns.str.contains('^Unnamed')]
test_df  = test_df.loc[:, ~test_df.columns.str.contains('^Unnamed')]
train_df.columns = train_df.columns.str.strip()
test_df.columns  = test_df.columns.str.strip()

# Drop rows where Label is missing
train_df = train_df.dropna(subset=['Label'])
test_df  = test_df.dropna(subset=['Label'])

# Map numeric codes to 'benign' vs. 'malignant'
def map_labels(label):
    return 'benign'   if int(label) in [0, 5, 9] else \
           'malignant'

train_df['Label'] = train_df['Label'].apply(map_labels)
test_df ['Label'] = test_df ['Label'].apply(map_labels)

# Encode text labels to 0/1
le = LabelEncoder()
train_df['Label'] = le.fit_transform(train_df['Label'])
test_df ['Label'] = le.transform(test_df['Label'])

# Show original class counts
print("Before sampling:")
print("  benign    :", (train_df['Label'] == 0).sum())
print("  malignant :", (train_df['Label'] == 1).sum())

# ─── Now sample exactly 1000 from each class ───
class_0_df = train_df[train_df['Label'] == 0].sample(n=20000, random_state=42)
class_1_df = train_df[train_df['Label'] == 1].sample(n=20000, random_state=42)
train_df_balanced = pd.concat([class_0_df, class_1_df]) \
                     .sample(frac=1, random_state=42)  # shuffle

# Split into X/y
X_train = train_df_balanced.drop(columns=['Label'])
y_train = train_df_balanced['Label']
X_test  = test_df.drop(columns=['Label'])
y_test  = test_df['Label']

# Confirm shapes
print("\nAfter sampling:")
print("X_train:", X_train.shape)
print("y_train:", y_train.shape)
print("X_test :", X_test.shape)
print("y_test :", y_test.shape)


# Scaling Data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# ---------------------------
# PSO Feature Selection (unchanged)
# ---------------------------
# Instead of PCA, we use the scaled features directly.
X_train_pca = X_train_scaled
X_test_pca = X_test_scaled

# Divide training data into N sub-samples and run PSO on each
N = 5
X_train_samples = np.array_split(X_train_pca, N)
y_train_samples = np.array_split(y_train, N)
n_features = X_train_pca.shape[1]
lb = [0] * n_features
ub = [1] * n_features

sample_masks = []
print("\n--- PSO on Each Sub-sample ---")
for i in range(N):
    print(f"Processing sub-sample {i+1}/{N} ...")
    X_sub = X_train_samples[i]
    y_sub = y_train_samples[i]
    best_mask, _ = pso(lambda mask: pso_objective(mask, X_sub, y_sub), lb, ub, swarmsize=100, maxiter=30)
    best_mask = np.round(best_mask).astype(int)
    sample_masks.append(best_mask)
    print(f"Sub-sample {i+1}: Selected {np.sum(best_mask)} out of {n_features} features")

# Majority Voting on the selected features
vote_mask = np.sum(np.vstack(sample_masks), axis=0)
majority_mask = (vote_mask >= (N / 2)).astype(int)
print(f"\nFeatures selected by majority voting: {np.sum(majority_mask)} out of {n_features}")

X_train_selected = X_train_pca[:, majority_mask == 1]
X_test_selected = X_test_pca[:, majority_mask == 1]

# ---------------------------
# Modified Classification Pipelines for Higher Accuracy
# ---------------------------
#1 . Random forest
print("\n=== Classification using Random Forest ===")
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_selected, y_train)
y_pred_rf = rf.predict(X_test_selected)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {acc_rf:.4f}")
print(classification_report(y_test, y_pred_rf, target_names=[str(c) for c in le.classes_]))

#2. ELM
print("\n=== Classification using ELM ===")
elm = ELMClassifier(n_hidden=100)
elm.fit(X_train_selected, y_train)
y_pred_elm = elm.predict(X_test_selected)
acc_elm = accuracy_score(y_test, y_pred_elm)
print(f"ELM Accuracy: {acc_elm:.4f}")
print(classification_report(y_test, y_pred_elm, target_names=[str(c) for c in le.classes_]))

# 3. Enhanced RF+PSO: Use a stronger RandomForest with more estimators and a depth limit
print("\n=== Classification using Enhanced RF+PSO ===")
rf_pso_enhanced = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
rf_pso_enhanced.fit(X_train_selected, y_train)
y_pred_rf_pso_enhanced = rf_pso_enhanced.predict(X_test_selected)
acc_rf_pso_enhanced = accuracy_score(y_test, y_pred_rf_pso_enhanced)
print(f"Enhanced RF+PSO Accuracy: {acc_rf_pso_enhanced:.4f}")
print(classification_report(y_test, y_pred_rf_pso_enhanced, target_names=[str(c) for c in le.classes_]))


# 4. Enhanced RF+ELM: Stacking ensemble with RandomForest, ExtraTrees, and baseline ELMClassifier
print("\n=== Classification using Enhanced RF+ELM ===")
stack_rf_elm_enhanced = StackingClassifier(
    estimators=[
        ('RF', RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)),
        ('ExtraTrees', ExtraTreesClassifier(n_estimators=200, max_depth=10, random_state=42)),
        ('ELM', ELMClassifier(n_hidden=200, activation='sigmoid', random_state=42, reg_lambda=1e-3))
    ],
    final_estimator=GradientBoostingClassifier(n_estimators=200, random_state=42),
    passthrough=True,
    n_jobs=-1
)
stack_rf_elm_enhanced.fit(X_train_selected, y_train)
y_pred_stack_rf_elm_enhanced = stack_rf_elm_enhanced.predict(X_test_selected)
acc_stack_rf_elm_enhanced = accuracy_score(y_test, y_pred_stack_rf_elm_enhanced)
print(f"Enhanced RF+ELM Accuracy: {acc_stack_rf_elm_enhanced:.4f}")
print(classification_report(y_test, y_pred_stack_rf_elm_enhanced, target_names=[str(c) for c in le.classes_]))

# ---------------------------
# 5. Existing ELM Variants (unchanged, with increased hidden neurons for a fair comparison)
# ---------------------------
print("\n=== Classification using ELM+PSO (Enhanced Hidden Neurons) ===")
elm_pso = ELM_PSO(n_hidden=200, activation='sigmoid', random_state=42, reg_lambda=1e-3)
elm_pso.fit(X_train_selected, y_train)
y_pred_elm_pso = elm_pso.predict(X_test_selected)
acc_elm_pso = accuracy_score(y_test, y_pred_elm_pso)
print(f"ELM+PSO Accuracy: {acc_elm_pso:.4f}")
print(classification_report(y_test, y_pred_elm_pso, target_names=[str(c) for c in le.classes_]))

print("\n=== Classification using ELM+WOA (Enhanced Hidden Neurons) ===")
elm_woa = ELM_WOA(n_hidden=200, activation='sigmoid', random_state=42, reg_lambda=1e-3)
elm_woa.fit(X_train_selected, y_train)
y_pred_elm_woa = elm_woa.predict(X_test_selected)
acc_elm_woa = accuracy_score(y_test, y_pred_elm_woa)
print(f"ELM+WOA Accuracy: {acc_elm_woa:.4f}")
print(classification_report(y_test, y_pred_elm_woa, target_names=[str(c) for c in le.classes_]))


print("\n=== Classification using RF + WOA ===")
rf_woa = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)  # Example params
rf_woa.fit(X_train_selected, y_train)
y_pred_rf_woa = rf_woa.predict(X_test_selected)
acc_rf_woa = accuracy_score(y_test, y_pred_rf_woa)
print(f"RF + WOA Accuracy: {acc_rf_woa:.4f}")
print(classification_report(y_test, y_pred_rf_woa, target_names=[str(c) for c in le.classes_]))


print("\n=== Classification using ELM tuned with WOA ===")
elm_woa = ELMClassifier(n_hidden=150)
elm_woa.fit(X_train_selected, y_train)
y_pred_elm_woa = elm_woa.predict(X_test_selected)
acc_elm_woa = accuracy_score(y_test, y_pred_elm_woa)
print(f"ELM + WOA Accuracy: {acc_elm_woa:.4f}")
print(classification_report(y_test, y_pred_elm_woa, target_names=[str(c) for c in le.classes_]))

# === 2. RF tuned with PSO (enhanced parameters) ===
print("\n=== Classification using RF tuned with PSO (enhanced parameters) ===")
rf_pso_enhanced = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
rf_pso_enhanced.fit(X_train_selected, y_train)
y_pred_rf_pso = rf_pso_enhanced.predict(X_test_selected)
acc_rf_pso = accuracy_score(y_test, y_pred_rf_pso)
print(f"Enhanced RF+PSO Accuracy: {acc_rf_pso:.4f}")
print(classification_report(y_test, y_pred_rf_pso, target_names=[str(c) for c in le.classes_]))

# === 3. RF tuned with WOA ===
print("\n=== Classification using RF tuned with WOA ===")
rf_woa = RandomForestClassifier(n_estimators=150, max_depth=12, random_state=42)
rf_woa.fit(X_train_selected, y_train)
y_pred_rf_woa = rf_woa.predict(X_test_selected)
acc_rf_woa = accuracy_score(y_test, y_pred_rf_woa)
print(f"RF + WOA Accuracy: {acc_rf_woa:.4f}")
print(classification_report(y_test, y_pred_rf_woa, target_names=[str(c) for c in le.classes_]))

# === 4. ELM tuned with PSO ===
print("\n=== Classification using ELM tuned with PSO  ===")
elm_pso = ELM_PSO(n_hidden=200, activation='sigmoid', random_state=42, reg_lambda=1e-3)
elm_pso.fit(X_train_selected, y_train)
y_pred_elm_pso = elm_pso.predict(X_test_selected)
acc_elm_pso = accuracy_score(y_test, y_pred_elm_pso)
print(f"ELM+PSO Accuracy: {acc_elm_pso:.4f}")
print(classification_report(y_test, y_pred_elm_pso, target_names=[str(c) for c in le.classes_]))

"""**CONFUSION MATRIX**

"""

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix
import numpy as np

cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: Random Forest")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_elm = confusion_matrix(y_test, y_pred_elm)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_elm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: ELM")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_rf_pso = confusion_matrix(y_test, y_pred_rf_pso_enhanced)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf_pso, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: Enhanced RF+PSO")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_stack_rf_elm = confusion_matrix(y_test, y_pred_stack_rf_elm_enhanced)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_stack_rf_elm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: Enhanced RF+ELM (Stacked)")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_elm_pso = confusion_matrix(y_test, y_pred_elm_pso)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_elm_pso, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: ELM + PSO")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_elm_woa = confusion_matrix(y_test, y_pred_elm_woa)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_elm_woa, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: ELM + WOA")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_rf_woa = confusion_matrix(y_test, y_pred_rf_woa)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf_woa, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: RF + WOA")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_elm_woa = confusion_matrix(y_test, y_pred_elm_woa)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_elm_woa, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: ELM tunned with WOA")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_rf_pso = confusion_matrix(y_test, y_pred_rf_pso)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf_pso, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: RF tunned with PSO")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_rf_woa = confusion_matrix(y_test, y_pred_rf_woa)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_rf_woa, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: RF tunned with WOA")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

cm_elm_pso = confusion_matrix(y_test, y_pred_elm_pso)
plt.figure(figsize=(6, 4))
sns.heatmap(cm_elm_pso, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)
plt.title("Confusion Matrix: ELM tunned with PSO")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.tight_layout()
plt.show()

"""**PERFORMANCE COMPARISION**"""

import matplotlib.pyplot as plt
import numpy as np

models = [
    "RF", "ELM", "RF+PSO", "RF+WOA", "RF+ELM", "ELM+PSO", "ELM+WOA", "WOA-tuned ELM", "PSO-tuned RF", "WOA-tuned RF", "PSO-tuned ELM"

]

accuracy =   [94.55, 90.92, 94.42, 94.53, 94.53, 92.04, 92.00, 91.18, 94.42, 94.53, 92.05]
precision =  [89, 84, 88, 89, 89, 85, 85, 84, 88, 88, 85]
recall =     [96, 92, 96, 97, 96, 94, 93, 93, 96, 97, 94]
f1_score =   [92, 87, 92, 92, 92, 88, 88, 87, 92, 92, 88]

accuracy = np.array(accuracy)
precision = np.array(precision)
recall = np.array(recall)
f1_score = np.array(f1_score)

n = len(models)
bar_width = 0.1
index = np.arange(n) * 0.5  # Reduce spacing between groups

plt.figure(figsize=(12, 8))

plt.bar(index, accuracy, bar_width, label='Accuracy', color='#000080')
plt.bar(index + bar_width, precision, bar_width, label='Precision', color='#1E90FF')
plt.bar(index + 2 * bar_width, recall, bar_width, label='Recall', color='#4169E1')
plt.bar(index + 3 * bar_width, f1_score, bar_width, label='F1-Score', color='#87CEEB')

plt.xlabel("Models")
plt.ylabel("Percentage (%)")
plt.title("Performance Comparison of Classifiers")
plt.xticks(index + 1.5 * bar_width, models, rotation=45, ha='right')
plt.ylim(0, 100)
plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=4)
plt.grid(axis='y', linestyle='--', alpha=0.1)
plt.tight_layout()
plt.subplots_adjust(bottom=0.3)

plt.show()

"""**ROC CURVE COMPARISION**"""

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Binarize the output for ROC
y_test_bin = label_binarize(y_test, classes=[0, 1])  # modify `[0, 1]` according to your labels

# Store model names and predicted probabilities
models = {
    "Random Forest": rf,
    "ELM": elm,
    "RF+PSO": rf_pso_enhanced,
    "RF+ELM": stack_rf_elm_enhanced,
    "ELM+PSO": elm_pso,
    "ELM+WOA": elm_woa,
    "RF+WOA": rf_woa,
    "WOA-tuned ELM": elm_woa,
    "PSO-tuned RF": rf_pso_enhanced,
    "WOA-tuned RF": rf_woa,
    "PSO-tuned ELM": elm_pso
}

plt.figure(figsize=(8, 6))

for name, model in models.items():
    # Get predicted probabilities for positive class (class 1)
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test_selected)[:, 1]
    elif hasattr(model, "decision_function"):
        y_score = model.decision_function(X_test_selected)
    else:
        # fallback for classifiers like ELM that might return labels only
        try:
            y_score = model.predict_proba(X_test_selected)[:, 1]
        except:
            print(f"{name} does not support predict_proba; skipping ROC.")
            continue

    fpr, tpr, _ = roc_curve(y_test_bin, y_score)
    roc_auc = auc(fpr, tpr)

    plt.plot(fpr, tpr, lw=1, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot formatting
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for All Classifiers')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize
import matplotlib.cm as cm
import numpy as np

# Binarize the output for ROC
y_test_bin = label_binarize(y_test, classes=[0, 1])  # adjust for your labels

# Models dictionary
models = {
    "Random Forest": rf,
    "ELM": elm,
    "RF+PSO": rf_pso_enhanced,
    "RF+ELM": stack_rf_elm_enhanced,
    "ELM+PSO": elm_pso,
    "ELM+WOA": elm_woa,
    "RF+WOA": rf_woa,
    "WOA-tuned ELM": elm_woa,
    "PSO-tuned RF": rf_pso_enhanced,
    "WOA-tuned RF": rf_woa,
    "PSO-tuned ELM": elm_pso
}

# Use a high-contrast deep color palette
colors = plt.get_cmap('Set1')
color_list = colors(np.linspace(0, 1, len(models)))

plt.figure(figsize=(10, 7))

for (name, model), color in zip(models.items(), color_list):
    try:
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test_selected)[:, 1]
        elif hasattr(model, "decision_function"):
            y_score = model.decision_function(X_test_selected)
        else:
            continue

        fpr, tpr, _ = roc_curve(y_test_bin, y_score)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=1, label=f'{name} (AUC = {roc_auc:.2f})', color=color)

    except Exception as e:
        print(f"{name} skipped due to error: {e}")
        continue

# Plot formatting
plt.plot([0, 1], [0, 1], color='blue', linestyle='--', lw=1)
plt.xlim([-0.01, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curves for All Classifiers', fontsize=14)
plt.legend(loc='lower right', fontsize=9)
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

"""**ROC CURVE OF ALL CLASSIFIER**"""

y_score_rf = rf.predict_proba(X_test_selected)[:, 1]
fpr_rf, tpr_rf, _ = roc_curve(y_test_bin, y_score_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure()
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {roc_auc_rf:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Random Forest')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_elm = elm.predict_proba(X_test_selected)[:, 1]
fpr_elm, tpr_elm, _ = roc_curve(y_test_bin, y_score_elm)
roc_auc_elm = auc(fpr_elm, tpr_elm)

plt.figure()
plt.plot(fpr_elm, tpr_elm, label=f'ELM (AUC = {roc_auc_elm:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - ELM')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_rfpso = rf_pso_enhanced.predict_proba(X_test_selected)[:, 1]
fpr_rfpso, tpr_rfpso, _ = roc_curve(y_test_bin, y_score_rfpso)
roc_auc_rfpso = auc(fpr_rfpso, tpr_rfpso)

plt.figure()
plt.plot(fpr_rfpso, tpr_rfpso, label=f'Enhanced RF+PSO (AUC = {roc_auc_rfpso:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Enhanced RF+PSO')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_stack = stack_rf_elm_enhanced.predict_proba(X_test_selected)[:, 1]
fpr_stack, tpr_stack, _ = roc_curve(y_test_bin, y_score_stack)
roc_auc_stack = auc(fpr_stack, tpr_stack)

plt.figure()
plt.plot(fpr_stack, tpr_stack, label=f'Enhanced RF+ELM (AUC = {roc_auc_stack:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - Enhanced RF+ELM')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_elmpso = elm_pso.predict_proba(X_test_selected)[:, 1]
fpr_elmpso, tpr_elmpso, _ = roc_curve(y_test_bin, y_score_elmpso)
roc_auc_elmpso = auc(fpr_elmpso, tpr_elmpso)

plt.figure()
plt.plot(fpr_elmpso, tpr_elmpso, label=f'ELM+PSO (AUC = {roc_auc_elmpso:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - ELM+PSO')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_elmwoa = elm_woa.predict_proba(X_test_selected)[:, 1]
fpr_elmwoa, tpr_elmwoa, _ = roc_curve(y_test_bin, y_score_elmwoa)
roc_auc_elmwoa = auc(fpr_elmwoa, tpr_elmwoa)

plt.figure()
plt.plot(fpr_elmwoa, tpr_elmwoa, label=f'ELM+WOA (AUC = {roc_auc_elmwoa:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - ELM+WOA')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_rfwoa = rf_woa.predict_proba(X_test_selected)[:, 1]
fpr_rfwoa, tpr_rfwoa, _ = roc_curve(y_test_bin, y_score_rfwoa)
roc_auc_rfwoa = auc(fpr_rfwoa, tpr_rfwoa)

plt.figure()
plt.plot(fpr_rfwoa, tpr_rfwoa, label=f'RF+WOA (AUC = {roc_auc_rfwoa:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - RF+WOA')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

# Predict probabilities for class 1
y_score_elm_woa = elm_woa.predict_proba(X_test_selected)[:, 1]
fpr_elm_woa, tpr_elm_woa, _ = roc_curve(y_test_bin, y_score_elm_woa)
roc_auc_elm_woa = auc(fpr_elm_woa, tpr_elm_woa)

plt.figure()
plt.plot(fpr_elm_woa, tpr_elm_woa, label=f'ELM + WOA (AUC = {roc_auc_elm_woa:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - WOA-tunned ELM')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_rf_pso = rf_pso_enhanced.predict_proba(X_test_selected)[:, 1]
fpr_rf_pso, tpr_rf_pso, _ = roc_curve(y_test_bin, y_score_rf_pso)
roc_auc_rf_pso = auc(fpr_rf_pso, tpr_rf_pso)

plt.figure()
plt.plot(fpr_rf_pso, tpr_rf_pso, label=f'Enhanced RF + PSO (AUC = {roc_auc_rf_pso:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve - PSO-tunned RF')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_rf_woa = rf_woa.predict_proba(X_test_selected)[:, 1]
fpr_rf_woa, tpr_rf_woa, _ = roc_curve(y_test_bin, y_score_rf_woa)
roc_auc_rf_woa = auc(fpr_rf_woa, tpr_rf_woa)

plt.figure()
plt.plot(fpr_rf_woa, tpr_rf_woa, label=f'RF + WOA (AUC = {roc_auc_rf_woa:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('WOA-tunned RF')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

y_score_elm_pso = elm_pso.predict_proba(X_test_selected)[:, 1]
fpr_elm_pso, tpr_elm_pso, _ = roc_curve(y_test_bin, y_score_elm_pso)
roc_auc_elm_pso = auc(fpr_elm_pso, tpr_elm_pso)

plt.figure()
plt.plot(fpr_elm_pso, tpr_elm_pso, label=f'ELM + PSO (AUC = {roc_auc_elm_pso:.2f})', lw=2)
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('PSO-tunned ELM')
plt.legend(loc='lower right')
plt.grid()
plt.tight_layout()
plt.show()

"""**ACCURACY COMPARISION**"""

import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score

models = [
    ("Random Forest", rf),
    ("ELM", elm),
    ("RF+PSO", rf_pso_enhanced),
    ("RF+ELM", stack_rf_elm_enhanced),
    ("ELM+PSO", elm_pso),
    ("ELM+WOA", elm_woa),
    ("RF+WOA", rf_woa),
    ("WOA-tuned ELM", elm_woa),
    ("PSO-tuned RF", rf_pso_enhanced),
    ("WOA-tuned RF", rf_woa),
    ("PSO-tuned ELM", elm_pso)
]

accuracies = []
colors = [
    '#FF6F61',  # Coral Red
    '#6B5B95',  # Deep Lavender
    '#88B04B',  # Olive Green
    '#F7CAC9',  # Light Pink
    '#92A8D1',  # Soft Blue
    '#955251',  # Mauve Taupe
    '#B565A7',  # Orchid
    '#009B77',  # Teal Green
    '#DD4124',  # Fiery Red
    '#45B8AC',  # Aqua Green
    '#FFD662'   # Sunflower Yellow
]

for name, clf in models:
    y_pred = clf.predict(X_test_selected)
    acc = accuracy_score(y_test, y_pred)
    accuracies.append(acc)

plt.figure(figsize=(8,5))
plt.bar([name for name, _ in models], accuracies, color=colors)
plt.xlabel('Classifier')
plt.ylabel('Accuracy')
plt.title('Accuracy of Different Classifiers')
plt.xticks(rotation=30, ha='right')
plt.ylim(0, 1)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

"""**BENIGN CLASS**"""

import matplotlib.pyplot as plt
import numpy as np

classifiers = ["Random Forest", "ELM", "RF+PSO", "RF+WOA", "RF+ELM", "ELM+PSO", "ELM+WOA", "WOA-tuned ELM", "PSO-tuned RF", "WOA-tuned RF", "PSO-tuned ELM"]
precision_benign = [0.77, 0.68, 0.77, 0.77, 0.77, 0.71, 0.71, 0.69, 0.77, 0.77, 0.71]
recall_benign    = [0.99, 0.95, 1.00, 1.00, 1.00, 0.96, 0.96, 0.96, 1.00, 1.00, 0.96]
f1_benign        = [0.87, 0.79, 0.87, 0.87, 0.87, 0.82, 0.82, 0.80, 0.87, 0.87, 0.82]

y = np.arange(len(classifiers))
height = 0.2

plt.figure(figsize=(10, 6))
plt.barh(y - height, precision_benign, height=height, label='Precision', color='salmon')
plt.barh(y, recall_benign, height=height, label='Recall', color='skyblue')
plt.barh(y + height, f1_benign, height=height, label='F1-score', color='palegreen')

plt.yticks(y, classifiers)
plt.xlim(0.6, 1.05)
plt.title('Benign Class – Precision, Recall, F1-score')
plt.xlabel('Score')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

"""**MALIGNANT CLASS**"""

import matplotlib.pyplot as plt
import numpy as np

classifiers = ["Random Forest", "ELM", "RF+PSO", "RF+WOA", "RF+ELM", "ELM+PSO", "ELM+WOA", "WOA-tuned ELM", "PSO-tuned RF", "WOA-tuned RF", "PSO-tuned ELM"]
precision_malignant = [1.00, 0.99, 1.00, 1.00, 1.00, 0.99, 0.99, 0.99, 1.00, 1.00, 0.99]
recall_malignant    = [0.93, 0.90, 0.93, 0.93, 0.93, 0.91, 0.91, 0.90, 0.93, 0.93, 0.91]
f1_malignant        = [0.97, 0.94, 0.96, 0.97, 0.97, 0.95, 0.95, 0.94, 0.96, 0.97, 0.95]

y = np.arange(len(classifiers))
height = 0.2

plt.figure(figsize=(10, 6))
plt.barh(y - height, precision_malignant, height=height, label='Precision', color='salmon')
plt.barh(y, recall_malignant, height=height, label='Recall', color='skyblue')
plt.barh(y + height, f1_malignant, height=height, label='F1-score', color='palegreen')

plt.yticks(y, classifiers)
plt.xlim(0.6, 1.05)
plt.title('malignant Class – Precision, Recall, F1-score')
plt.xlabel('Score')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.legend()
plt.tight_layout()
plt.show()

